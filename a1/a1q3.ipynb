{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6ALfTMdkkR0"
   },
   "source": [
    "# Implementing Neural Network Training with Different Optimizers\n",
    "\n",
    "### Objective:\n",
    "The goal of this assignment is to implement a small neural network from scratch and train it using three different optimization algorithms: Stochastic Gradient Descent (SGD), SGD with Momentum, and SGD with AdaGrad. You will need to compare the performance of these optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtxIed1pk-P8"
   },
   "source": [
    "## 1- Dataset\n",
    "\n",
    "### Digits\n",
    "\n",
    "This dataset, sourced from sklearn, consists of 1797 images, each sized 8x8 pixels. Every image, like the example shown below, depicts a handwritten digit. To utilize an 8x8 image, it must first be transformed into a feature vector of length 64.\n",
    "\n",
    "The task with this dataset is to classify each digit, with a total of 10 classes.\n",
    "\n",
    "The targets are one-hot encoded, i.e. each digit is transformed into a 10-dimensional vector with the element corresponding to the digit set to 1, and the other elements set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:14:54.062336Z",
     "iopub.status.busy": "2024-06-15T02:14:54.062088Z",
     "iopub.status.idle": "2024-06-15T02:15:02.563405Z",
     "shell.execute_reply": "2024-06-15T02:15:02.562635Z"
    },
    "id": "ZaQUtmPR7Mqy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "print(\"dataset's dimensions:\", digits.data.shape)\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.567381Z",
     "iopub.status.busy": "2024-06-15T02:15:02.567038Z",
     "iopub.status.idle": "2024-06-15T02:15:02.572266Z",
     "shell.execute_reply": "2024-06-15T02:15:02.571542Z"
    },
    "id": "VeM0eCK-lq-1"
   },
   "outputs": [],
   "source": [
    "# this function loads train and test sets from digits dataset\n",
    "def load_data():\n",
    "    digits = load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6v3VQwGl8xr"
   },
   "source": [
    "## 2- Neural Network Architecture:\n",
    "* Implement a simple feedforward neural network with one hidden layer.\n",
    "* Use sigmoid activation for the hidden layer and softmax for the output layer.\n",
    "* Feel free to add any necessary functions to the class below.\n",
    "* See figure 2 in assignment pdf for a diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.576025Z",
     "iopub.status.busy": "2024-06-15T02:15:02.575643Z",
     "iopub.status.idle": "2024-06-15T02:15:02.588152Z",
     "shell.execute_reply": "2024-06-15T02:15:02.587317Z"
    },
    "id": "F_z9piRgsQpE"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a - np.max(a, axis=1, keepdims=True))\n",
    "    return exp_a / np.sum(exp_a, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    z = z[:, :, np.newaxis]\n",
    "    z_diag = z * np.eye(z.shape[1])\n",
    "    return z_diag - np.einsum('ijk,ikl->ijl', z, z.transpose(0, 2, 1))\n",
    "\n",
    "\n",
    "# Neural network forward and backward propagation\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        self.dW1 = np.zeros((input_size, hidden_size))\n",
    "        self.db1 = np.zeros((1, hidden_size))\n",
    "        self.dW2 = np.zeros((hidden_size, output_size))\n",
    "        self.db2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Implement forward pass\n",
    "        # You should store the intermediate values as fields to be used by the backward pass\n",
    "        # The forward pass must work for all batch sizes\n",
    "\n",
    "        # X: the x values, batched along dimension zero\n",
    "        # return: z2\n",
    "        self.a1 = np.dot(X, self.W1) + self.b1\n",
    "        self.z1 = sigmoid(self.a1)\n",
    "        self.a2 = np.dot(self.z1, self.W2) + self.b2\n",
    "        self.z2 = softmax(self.a2)\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        # Implement backward pass, assuming the MSE loss (see section 3 for clarification)\n",
    "        # Use the intermediate values calculated from the forward pass,\n",
    "        # and store the gradients in fields\n",
    "        # The backward pass must also work for all batch sizes\n",
    "\n",
    "        # X: the x values, batched along dimension zero\n",
    "        # y: batched target values\n",
    "        # return: None\n",
    "        dz2 = 2 * (self.z2 - y) / y.shape[0]\n",
    "        d2 = np.einsum('ij,ijk->ik', dz2, softmax_derivative(self.z2))\n",
    "\n",
    "        # Gradients for W2 and b2\n",
    "        self.dW2 = np.dot(self.z1.T, d2)\n",
    "        self.db2 = np.sum(d2, axis=0, keepdims=True)\n",
    "\n",
    "        d1 = np.dot(d2, self.W2.T) * sigmoid_derivative(self.z1)\n",
    "\n",
    "        # Gradients for W1 and b1\n",
    "        self.dW1 = np.dot(X.T, d1)\n",
    "        self.db1 = np.sum(d1, axis=0, keepdims=True)\n",
    "\n",
    "    def get_params_and_grads(self):\n",
    "        # Return parameters and corresponding gradients\n",
    "        params = [self.W1, self.b1, self.W2, self.b2]\n",
    "        grads = [self.dW1, self.db1, self.dW2, self.db2]\n",
    "        return params, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS2WtmWLTAKv"
   },
   "source": [
    "## 3- Helper Functions\n",
    "Implement the following helper functions (the `plot_all_results` function is already implemented):\n",
    "\n",
    "* `mean_squared_error(predictions, targets)`: This function receives the predicted values from the network and the target values, calculates the mean squared loss, and returns the loss value. For clarification, you should take the mean of the squared error over the samples in the batch, and *not* over the outputs of each sample\n",
    "\n",
    "* `compute_accuracy(predictions, targets)`: This function receives the predicted values and the target values, and returns the accuracy of the predictions. The predicted value is the index with the highest network output (z2) value.\n",
    "\n",
    "* Feel free to add any other necessary helper functions here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.591974Z",
     "iopub.status.busy": "2024-06-15T02:15:02.591599Z",
     "iopub.status.idle": "2024-06-15T02:15:02.599920Z",
     "shell.execute_reply": "2024-06-15T02:15:02.599228Z"
    },
    "id": "IoZ24qQES_h8"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, targets):\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(targets, axis=1)\n",
    "    return np.mean(pred_classes == true_classes)\n",
    "\n",
    "\n",
    "def plot_all_results(all_losses, all_accuracies, all_labels):\n",
    "    if len(all_losses) != len(all_accuracies):\n",
    "        raise ValueError(\n",
    "            \"all_losses length must be equal to all_accuracies length\")\n",
    "\n",
    "    if len(all_losses) != len(all_labels):\n",
    "        raise ValueError(\n",
    "            \"all_labels length must be equal to all_losses length\")\n",
    "\n",
    "    epochs = len(all_losses[0])\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(len(all_losses)):\n",
    "        plt.plot(range(1, epochs + 1), all_losses[i], label=all_labels[i])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training loss')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(len(all_losses)):\n",
    "        plt.plot(range(1, epochs + 1), all_accuracies[i], label=all_labels[i])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Training accuracy')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# You may add helper functions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqtkTxdO7xrJ"
   },
   "source": [
    "## 4- Optimizer Implementations:\n",
    "Implement the following optimization algorithms:\n",
    "\n",
    " * Stochastic Gradient Descent with minibatches (SGD)\n",
    " * SGD with Momentum\n",
    " * SGD with AdaGrad\n",
    "\n",
    "Note that you must update each param in-place using `+=` or `-=` so that the weights of the network are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.603619Z",
     "iopub.status.busy": "2024-06-15T02:15:02.603244Z",
     "iopub.status.idle": "2024-06-15T02:15:02.612403Z",
     "shell.execute_reply": "2024-06-15T02:15:02.611689Z"
    },
    "id": "xCJaMUd7wwh9"
   },
   "outputs": [],
   "source": [
    "# Optimizer implementations (SGD, SGD with Momentum, AdaGrad)\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, grads):\n",
    "        # Perform one step of SGD\n",
    "        for param, grad in zip(self.params, grads):\n",
    "            param -= self.lr * grad\n",
    "\n",
    "\n",
    "class SGD_Momentum():\n",
    "    def __init__(self, params, learning_rate, alpha):\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.v = [np.zeros_like(param) for param in params]\n",
    "\n",
    "    def step(self, grads):\n",
    "        # Perform one step of SGD with momentum\n",
    "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
    "            self.v[i] = self.alpha * self.v[i] - self.lr * grad\n",
    "            param += self.v[i]\n",
    "\n",
    "\n",
    "class SGD_AdaGrad():\n",
    "    def __init__(self, params, learning_rate, delta):\n",
    "        self.params = params\n",
    "        self.lr = learning_rate\n",
    "        self.delta = delta\n",
    "        self.r = [np.zeros_like(param) for param in params]\n",
    "\n",
    "    def step(self, grads):\n",
    "        # Perform one step of SGD with adagrad\n",
    "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
    "            self.r[i] += grad ** 2\n",
    "            param -= self.lr * grad / (self.delta + np.sqrt(self.r[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-IYpGMV9dY8"
   },
   "source": [
    "## 5- Training Loop:\n",
    "\n",
    "The `train` function is used to train your neural network. It returns the training loss and accuracy after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.617680Z",
     "iopub.status.busy": "2024-06-15T02:15:02.617215Z",
     "iopub.status.idle": "2024-06-15T02:15:02.627318Z",
     "shell.execute_reply": "2024-06-15T02:15:02.626098Z"
    },
    "id": "w7sB8aQS9VPw"
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def gen_batches(data, labels, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size], labels[i:i+batch_size]\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(network, data, optimizer, epochs, batch_size):\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random_indices = np.random.permutation(list(range(X_train.shape[0])))\n",
    "        X_train = X_train[random_indices]\n",
    "        y_train = y_train[random_indices]\n",
    "        for x, y in gen_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass\n",
    "            output = network.forward(x)\n",
    "\n",
    "            # Backward pass\n",
    "            network.backward(x, y)\n",
    "\n",
    "            # Get parameters and gradients\n",
    "            params, grads = network.get_params_and_grads()\n",
    "\n",
    "            # Update parameters using the chosen optimizer\n",
    "            optimizer.step(grads)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "        output = network.forward(X_test)\n",
    "        train_loss = mean_squared_error(output, y_test)\n",
    "        train_accuracy = compute_accuracy(output, y_test)\n",
    "\n",
    "        test_losses.append(train_loss)\n",
    "        test_accuracies.append(train_accuracy)\n",
    "\n",
    "        if epoch % 20 == 19:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    return test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN7zvKO8-lfG"
   },
   "source": [
    "## 6- Main function\n",
    "Use the following main function to train your neural network based on sgd, sgd-momentum, and AdaGrad. The following hyperparameters are used in training:\n",
    "\n",
    "* batch_size = 128\n",
    "* input_size = 64\n",
    "* hidden_size = 20\n",
    "* output_size = 10\n",
    "* learning_rate = see LR on graph legend\n",
    "* For SGD with momentum, alpha = 0.9\n",
    "* For SGD with AdaGrad, delta = 10^-8\n",
    "* epochs = 200\n",
    "\n",
    "Compare the performance of the 5 different training passes, and discuss the impact of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:02.631876Z",
     "iopub.status.busy": "2024-06-15T02:15:02.631397Z",
     "iopub.status.idle": "2024-06-15T02:15:12.768593Z",
     "shell.execute_reply": "2024-06-15T02:15:12.767768Z"
    },
    "id": "IKcfTHJi-l6p"
   },
   "outputs": [],
   "source": [
    "# Main function to run the experiments\n",
    "def main():\n",
    "    data = load_data()\n",
    "    batch_size = 128\n",
    "    input_size = 64  # For the digits dataset (8x8 images flattened)\n",
    "    hidden_size = 20\n",
    "    output_size = 10\n",
    "    epochs = 200\n",
    "\n",
    "    print(\"Training with SGD LR 0.5\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    test_losses_sgd, test_accuracies_sgd = train(network, data, SGD(\n",
    "        network.get_params_and_grads()[0], 0.5), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with SGD LR 0.2\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    test_losses_sgd_b, test_accuracies_sgd_b = train(network, data, SGD(\n",
    "        network.get_params_and_grads()[0], 0.2), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with SGD with Momentum LR 0.05\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    test_losses_momentum, test_accuracies_momentum = train(network, data, SGD_Momentum(\n",
    "        network.get_params_and_grads()[0], 0.05, 0.9), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with AdaGrad LR 0.05\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    test_losses_adagrad, test_accuracies_adagrad = train(network, data, SGD_AdaGrad(\n",
    "        network.get_params_and_grads()[0], 0.05, 1e-8), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with AdaGrad LR 0.01\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    test_losses_adagrad_b, test_accuracies_adagrad_b = train(network, data, SGD_AdaGrad(\n",
    "        network.get_params_and_grads()[0], 0.01, 1e-8), epochs, batch_size)\n",
    "\n",
    "    # Compare train losses and train accuracies\n",
    "    all_losses = [test_losses_sgd, test_losses_sgd_b,\n",
    "                  test_losses_momentum, test_losses_adagrad, test_losses_adagrad_b]\n",
    "    all_accuracies = [test_accuracies_sgd, test_accuracies_sgd_b,\n",
    "                      test_accuracies_momentum, test_accuracies_adagrad, test_accuracies_adagrad_b]\n",
    "    all_labels = [\"SGD LR 0.5\", \"SGD LR 0.2\",\n",
    "                  \"SGD-Momentum LR 0.05\", \"AdaGrad LR 0.05\", \"AdaGrad LR 0.01\"]\n",
    "    plot_all_results(all_losses, all_accuracies, all_labels)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Increasing Layers\n",
    "\n",
    "Instead of using 2-layer neural networks. We can increase number of layers to three and observe the loss again based on sgd. The newly added layer should adopt the same hidden size.\n",
    "\n",
    "We are sticking to the same hyper-parameters as before.\n",
    "\n",
    "* Plot the loss and accuracy.\n",
    "\n",
    "* Discuss the impact of the increased layers in the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T02:15:12.777142Z",
     "iopub.status.busy": "2024-06-15T02:15:12.776924Z",
     "iopub.status.idle": "2024-06-15T02:15:22.351908Z",
     "shell.execute_reply": "2024-06-15T02:15:22.351076Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(\n",
    "            input_size, hidden_size1) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        self.W2 = np.random.randn(\n",
    "            hidden_size1, hidden_size2) * np.sqrt(2. / hidden_size1)\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        self.W3 = np.random.randn(\n",
    "            hidden_size2, output_size) * np.sqrt(2. / hidden_size2)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "        self.dW1 = np.zeros((input_size, hidden_size1))\n",
    "        self.db1 = np.zeros((1, hidden_size1))\n",
    "        self.dW2 = np.zeros((hidden_size1, hidden_size2))\n",
    "        self.db2 = np.zeros((1, hidden_size2))\n",
    "        self.dW3 = np.zeros((hidden_size2, output_size))\n",
    "        self.db3 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Implement forward pass\n",
    "        self.a1 = np.dot(X, self.W1) + self.b1\n",
    "        self.z1 = sigmoid(self.a1)\n",
    "        self.a2 = np.dot(self.z1, self.W2) + self.b2\n",
    "        self.z2 = sigmoid(self.a2)\n",
    "        self.a3 = np.dot(self.z2, self.W3) + self.b3\n",
    "        self.z3 = softmax(self.a3)\n",
    "        return self.z3\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        dz3 = 2 * (self.z3 - y) / y.shape[0]\n",
    "        d3 = np.einsum('ij,ijk->ik', dz3, softmax_derivative(self.z3))\n",
    "\n",
    "        # Gradients for W3 and b3\n",
    "        self.dW3 = np.dot(self.z2.T, d3)\n",
    "        self.db3 = np.sum(d3, axis=0, keepdims=True)\n",
    "\n",
    "        d2 = np.dot(d3, self.W3.T) * sigmoid_derivative(self.z2)\n",
    "\n",
    "        # Gradients for W2 and b2\n",
    "        self.dW2 = np.dot(self.z1.T, d2)\n",
    "        self.db2 = np.sum(d2, axis=0, keepdims=True)\n",
    "\n",
    "        d1 = np.dot(d2, self.W2.T) * sigmoid_derivative(self.z1)\n",
    "\n",
    "        # Gradients for W1 and b1\n",
    "        self.dW1 = np.dot(X.T, d1)\n",
    "        self.db1 = np.sum(d1, axis=0, keepdims=True)\n",
    "\n",
    "    def get_params_and_grads(self):\n",
    "        # Return parameters and corresponding gradients\n",
    "        params = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "        grads = [self.dW1, self.db1, self.dW2, self.db2, self.dW3, self.db3]\n",
    "        return params, grads\n",
    "\n",
    "\n",
    "# Main function to run the experiments\n",
    "def main():\n",
    "    data = load_data()\n",
    "    batch_size = 128\n",
    "    input_size = 64  # For the digits dataset (8x8 images flattened)\n",
    "    hidden_size = 20\n",
    "    output_size = 10\n",
    "    epochs = 200\n",
    "\n",
    "    print(\"Training with SGD LR 0.5\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, hidden_size, output_size)\n",
    "    test_losses_sgd, test_accuracies_sgd = train(network, data, SGD(\n",
    "        network.get_params_and_grads()[0], 0.5), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with SGD LR 0.2\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, hidden_size, output_size)\n",
    "    test_losses_sgd_b, test_accuracies_sgd_b = train(network, data, SGD(\n",
    "        network.get_params_and_grads()[0], 0.2), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with SGD with Momentum LR 0.05\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, hidden_size, output_size)\n",
    "    test_losses_momentum, test_accuracies_momentum = train(network, data, SGD_Momentum(\n",
    "        network.get_params_and_grads()[0], 0.05, 0.9), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with AdaGrad LR 0.05\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, hidden_size, output_size)\n",
    "    test_losses_adagrad, test_accuracies_adagrad = train(network, data, SGD_AdaGrad(\n",
    "        network.get_params_and_grads()[0], 0.05, 1e-8), epochs, batch_size)\n",
    "\n",
    "    print(\"Training with AdaGrad LR 0.01\")\n",
    "    network = NeuralNetwork(input_size, hidden_size, hidden_size, output_size)\n",
    "    test_losses_adagrad_b, test_accuracies_adagrad_b = train(network, data, SGD_AdaGrad(\n",
    "        network.get_params_and_grads()[0], 0.01, 1e-8), epochs, batch_size)\n",
    "\n",
    "    # Compare train losses and train accuracies\n",
    "    all_losses = [test_losses_sgd, test_losses_sgd_b,\n",
    "                  test_losses_momentum, test_losses_adagrad, test_losses_adagrad_b]\n",
    "    all_accuracies = [test_accuracies_sgd, test_accuracies_sgd_b,\n",
    "                      test_accuracies_momentum, test_accuracies_adagrad, test_accuracies_adagrad_b]\n",
    "    all_labels = [\"SGD LR 0.5\", \"SGD LR 0.2\",\n",
    "                  \"SGD-Momentum LR 0.05\", \"AdaGrad LR 0.05\", \"AdaGrad LR 0.01\"]\n",
    "    plot_all_results(all_losses, all_accuracies, all_labels)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
