{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ALfTMdkkR0"
      },
      "source": [
        "# Implementing Neural Network Training with Different Optimizers\n",
        "\n",
        "### Objective:\n",
        "The goal of this assignment is to implement a small neural network from scratch and train it using three different optimization algorithms: Stochastic Gradient Descent (SGD), SGD with Momentum, and SGD with AdaGrad. You will need to compare the performance of these optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtxIed1pk-P8"
      },
      "source": [
        "## 1- Dataset\n",
        "\n",
        "### Digits\n",
        "\n",
        "This dataset, sourced from sklearn, consists of 1797 images, each sized 8x8 pixels. Every image, like the example shown below, depicts a handwritten digit. To utilize an 8x8 image, it must first be transformed into a feature vector of length 64.\n",
        "\n",
        "The task with this dataset is to classify each digit, with a total of 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaQUtmPR7Mqy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "digits = load_digits()\n",
        "print(\"dataset's dimensions:\", digits.data.shape)\n",
        "plt.gray()\n",
        "plt.matshow(digits.images[0])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeM0eCK-lq-1"
      },
      "outputs": [],
      "source": [
        "# this function loads train and test sets from digits dataset\n",
        "def load_data():\n",
        "    digits = load_digits()\n",
        "    X = digits.data\n",
        "    y = digits.target\n",
        "\n",
        "    enc = OneHotEncoder()\n",
        "    y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6v3VQwGl8xr"
      },
      "source": [
        "## 2- Neural Network Architecture:\n",
        "* Implement a simple feedforward neural network with one hidden layer.\n",
        "* Use sigmoid activation for the hidden layer and softmax for the output layer.\n",
        "* Feel free to add any necessary functions to the class below.\n",
        "* See figure 2 in assignment pdf for a diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_z9piRgsQpE"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(s):\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        self.dW1 = np.zeros((input_size, hidden_size))\n",
        "        self.db1 = np.zeros((1, hidden_size))\n",
        "        self.dW2 = np.zeros((hidden_size, output_size))\n",
        "        self.db2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Implement forward pass\n",
        "        # You should store the intermediate values as fields to be used by the backward pass\n",
        "        # The forward pass must work for all batch sizes\n",
        "\n",
        "        # X: the x values, batched along dimension zero\n",
        "        # return: z2\n",
        "        self.a1 = np.dot(X, self.W1) + self.b1\n",
        "        self.z1 = sigmoid(self.a1)\n",
        "        self.a2 = np.dot(self.z1, self.W2) + self.b2\n",
        "        self.z2 = softmax(self.a2)\n",
        "        return self.z2\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        # Implement backward pass, assuming the MSE loss\n",
        "        # Use the intermediate values calculated from the forward pass,\n",
        "        # and store the gradients in fields\n",
        "        # The backward pass only need to work for batch size 1\n",
        "\n",
        "        # X: the x values, batched along dimension zero\n",
        "        # y: batched target values\n",
        "        # return: None\n",
        "        d2 = (self.z2 - y) / y.shape[0]\n",
        "\n",
        "        # Gradients for W2 and b2\n",
        "        self.dW2 = np.dot(self.z1.T, d2)\n",
        "        self.db2 = np.sum(d2, axis=0, keepdims=True)\n",
        "\n",
        "        d1 = np.dot(d2, self.W2.T) * sigmoid_derivative(self.z1)\n",
        "\n",
        "        # Gradients for W1 and b1\n",
        "        self.dW1 = np.dot(X.T, d1)\n",
        "        self.db1 = np.sum(d1, axis=0, keepdims=True)\n",
        "\n",
        "    def get_params_and_grads(self):\n",
        "        # Return parameters and corresponding gradients\n",
        "        params = [self.W1, self.b1, self.W2, self.b2]\n",
        "        grads = [self.dW1, self.db1, self.dW2, self.db2]\n",
        "        return params, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS2WtmWLTAKv"
      },
      "source": [
        "## 3- Helper Functions\n",
        "Implement the following helper functions (the \"plot_all_results\" function is already implemented):\n",
        "\n",
        "* \"mean_squared_error(predictions, targets)\": This function receives the predicted values from the network and the target values, calculates the mean squared loss, and returns the loss value.\n",
        "\n",
        "* \"compute_accuracy(predictions, targets)\": This function receives the predicted values and the target values, and returns the accuracy of the predictions.\n",
        "\n",
        "* Feel free to add any other necessary helper functions here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoZ24qQES_h8"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(predictions, targets):\n",
        "    return np.mean((predictions - targets) ** 2)\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, targets):\n",
        "    pred_classes = np.argmax(predictions, axis=1)\n",
        "    true_classes = np.argmax(targets, axis=1)\n",
        "    return np.mean(pred_classes == true_classes)\n",
        "\n",
        "\n",
        "def plot_all_results(all_losses, all_accuracies, all_labels):\n",
        "    if len(all_losses) != len(all_accuracies):\n",
        "        raise ValueError(\n",
        "            \"all_losses length must be equal to all_accuracies length\")\n",
        "\n",
        "    if len(all_losses) != len(all_labels):\n",
        "        raise ValueError(\n",
        "            \"all_labels length must be equal to all_losses length\")\n",
        "\n",
        "    epochs = len(all_losses[0])\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i in range(len(all_losses)):\n",
        "        plt.plot(range(1, epochs + 1), all_losses[i], label=all_labels[i])\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Loss behaviours')\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i in range(len(all_losses)):\n",
        "        plt.plot(range(1, epochs + 1), all_accuracies[i], label=all_labels[i])\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Accuracy behaviours')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqtkTxdO7xrJ"
      },
      "source": [
        "## 4- Optimizer Implementations:\n",
        "Implement the following optimization algorithms:\n",
        "\n",
        " * Stochastic Gradient Descent (SGD)\n",
        " * SGD with Momentum\n",
        " * SGD with AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCJaMUd7wwh9"
      },
      "outputs": [],
      "source": [
        "# Optimizer implementations (SGD, SGD with Momentum, AdaGrad)\n",
        "\n",
        "class SGD():\n",
        "    def __init__(self, params, learning_rate):\n",
        "        self.params = params\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def step(self, grads):\n",
        "        # Perform one step of SGD\n",
        "        for param, grad in zip(self.params, grads):\n",
        "            param -= self.lr * grad\n",
        "\n",
        "\n",
        "class SGD_Momentum():\n",
        "    def __init__(self, params, learning_rate, alpha):\n",
        "        self.params = params\n",
        "        self.lr = learning_rate\n",
        "        self.alpha = alpha\n",
        "        self.velocity = [np.zeros_like(param) for param in params]\n",
        "\n",
        "    def step(self, grads):\n",
        "        # Perform one step of SGD with momentum\n",
        "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
        "            self.velocity[i] = self.alpha * self.velocity[i] - self.lr * grad\n",
        "            param += self.velocity[i]\n",
        "\n",
        "\n",
        "class SGD_AdaGrad():\n",
        "    def __init__(self, params, learning_rate, delta):\n",
        "        self.params = params\n",
        "        self.lr = learning_rate\n",
        "        self.delta = delta\n",
        "        self.r = [np.zeros_like(param) for param in params]\n",
        "\n",
        "    def step(self, grads):\n",
        "        # Perform one step of SGD with adagrad\n",
        "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
        "            self.r[i] += grad ** 2\n",
        "            param -= self.lr * grad / (np.sqrt(self.r[i]) + self.delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-IYpGMV9dY8"
      },
      "source": [
        "## 5- Training Loop:\n",
        "\n",
        "Use the following training function to train your neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7sB8aQS9VPw"
      },
      "outputs": [],
      "source": [
        "# batch generator\n",
        "def gen_batches(data, labels, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i+batch_size], labels[i:i+batch_size]\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(network, data, optimizer, epochs, batch_size):\n",
        "    X_train, y_train, X_test, y_test = data\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        random_indices = np.random.permutation(list(range(X_train.shape[0])))\n",
        "        X_train = X_train[random_indices]\n",
        "        y_train = y_train[random_indices]\n",
        "        for x, y in gen_batches(X_train, y_train, batch_size):\n",
        "            # Forward pass\n",
        "            output = network.forward(x)\n",
        "\n",
        "            # Backward pass\n",
        "            network.backward(x, y)\n",
        "\n",
        "            # Get parameters and gradients\n",
        "            params, grads = network.get_params_and_grads()\n",
        "\n",
        "            # Update parameters using the chosen optimizer\n",
        "            optimizer.step(grads)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        X_test = X_train\n",
        "        y_test = y_train\n",
        "        output = network.forward(X_test)\n",
        "        train_loss = mean_squared_error(output, y_test)\n",
        "        train_accuracy = compute_accuracy(output, y_test)\n",
        "\n",
        "        test_losses.append(train_loss)\n",
        "        test_accuracies.append(train_accuracy)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return test_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN7zvKO8-lfG"
      },
      "source": [
        "## 6- Main function\n",
        "Use the following main function to train your neural network based on sgd, sgd-momentum, and AdaGrad. The following hyperparameters are used in training:\n",
        "\n",
        "* batch_size = 128\n",
        "* input_size = 64\n",
        "* hidden_size = 20\n",
        "* output_size = 10\n",
        "* learning_rate = 0.005\n",
        "* epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKcfTHJi-l6p"
      },
      "outputs": [],
      "source": [
        "# Main function to run the experiments\n",
        "def main():\n",
        "    data = load_data()\n",
        "    batch_size = 128\n",
        "    input_size = 64  # For the digits dataset (8x8 images flattened)\n",
        "    hidden_size = 20\n",
        "    output_size = 10\n",
        "    learning_rate = 0.005\n",
        "    epochs = 100\n",
        "\n",
        "    print(\"Training with SGD LR 0.005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "    test_losses_sgd, test_accuracies_sgd = train(network, data, SGD(\n",
        "        network.get_params_and_grads()[0], learning_rate), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with SGD LR 0.05\")\n",
        "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "    test_losses_sgd_b, test_accuracies_sgd_b = train(network, data, SGD(\n",
        "        network.get_params_and_grads()[0], learning_rate * 10), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with SGD with Momentum LR 0.0005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "    test_losses_momentum, test_accuracies_momentum = train(network, data, SGD_Momentum(\n",
        "        network.get_params_and_grads()[0], learning_rate / 10, 0.9), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with AdaGrad LR 0.005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "    test_losses_adagrad, test_accuracies_adagrad = train(network, data, SGD_AdaGrad(\n",
        "        network.get_params_and_grads()[0], learning_rate, 1e-8), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with AdaGrad LR 0.05\")\n",
        "    network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "    test_losses_adagrad_b, test_accuracies_adagrad_b = train(network, data, SGD_AdaGrad(\n",
        "        network.get_params_and_grads()[0], learning_rate * 10, 1e-8), epochs, batch_size)\n",
        "\n",
        "    # Compare train losses and train accuracies\n",
        "    all_losses = [test_losses_sgd, test_losses_sgd_b,\n",
        "                  test_losses_momentum, test_losses_adagrad, test_losses_adagrad_b]\n",
        "    all_accuracies = [test_accuracies_sgd, test_accuracies_sgd_b,\n",
        "                      test_accuracies_momentum, test_accuracies_adagrad, test_accuracies_adagrad_b]\n",
        "    all_labels = [\"SGD LR 0.005\", \"SGD LR 0.05\",\n",
        "                  \"SGD-Momentum LR 0.0005\", \"AdaGrad LR 0.005\", \"AdaGrad LR 0.05\"]\n",
        "    plot_all_results(all_losses, all_accuracies, all_labels)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Increasing Layers\n",
        "\n",
        "Instead of using 2-layer neural networks. We can increase the layer size to three and observe the loss again based on sgd. The newly added layer should adopt the same hidden size.\n",
        "\n",
        "We are sticking to the same hyper-parameters as before.\n",
        "\n",
        "* Plot the loss and accuracy.\n",
        "\n",
        "* Discuss the impact of the increased layers in the pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please implement the 3-layer neural network and run the main function again to plot the curve.\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "\n",
        "def relu_derivative(s):\n",
        "    return np.where(s > 0, 1, 0)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(\n",
        "            input_size, hidden_size1) * np.sqrt(2. / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size1))\n",
        "        self.W2 = np.random.randn(\n",
        "            hidden_size1, hidden_size2) * np.sqrt(2. / hidden_size1)\n",
        "        self.b2 = np.zeros((1, hidden_size2))\n",
        "        self.W3 = np.random.randn(\n",
        "            hidden_size2, output_size) * np.sqrt(2. / hidden_size2)\n",
        "        self.b3 = np.zeros((1, output_size))\n",
        "\n",
        "        self.dW1 = np.zeros((input_size, hidden_size1))\n",
        "        self.db1 = np.zeros((1, hidden_size1))\n",
        "        self.dW2 = np.zeros((hidden_size1, hidden_size2))\n",
        "        self.db2 = np.zeros((1, hidden_size2))\n",
        "        self.dW3 = np.zeros((hidden_size2, output_size))\n",
        "        self.db3 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Implement forward pass\n",
        "        self.a1 = np.dot(X, self.W1) + self.b1\n",
        "        self.z1 = relu(self.a1)\n",
        "        self.a2 = np.dot(self.z1, self.W2) + self.b2\n",
        "        self.z2 = relu(self.a2)\n",
        "        self.a3 = np.dot(self.z2, self.W3) + self.b3\n",
        "        self.z3 = softmax(self.a3)\n",
        "        return self.z3\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        d3 = (self.z3 - y) / y.shape[0]\n",
        "\n",
        "        # Gradients for W3 and b3\n",
        "        self.dW3 = np.dot(self.z2.T, d3)\n",
        "        self.db3 = np.sum(d3, axis=0, keepdims=True)\n",
        "\n",
        "        d2 = np.dot(d3, self.W3.T) * relu_derivative(self.z2)\n",
        "\n",
        "        # Gradients for W2 and b2\n",
        "        self.dW2 = np.dot(self.z1.T, d2)\n",
        "        self.db2 = np.sum(d2, axis=0, keepdims=True)\n",
        "\n",
        "        d1 = np.dot(d2, self.W2.T) * relu_derivative(self.z1)\n",
        "\n",
        "        # Gradients for W1 and b1\n",
        "        self.dW1 = np.dot(X.T, d1)\n",
        "        self.db1 = np.sum(d1, axis=0, keepdims=True)\n",
        "\n",
        "    def get_params_and_grads(self):\n",
        "        # Return parameters and corresponding gradients\n",
        "        params = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
        "        grads = [self.dW1, self.db1, self.dW2, self.db2, self.dW3, self.db3]\n",
        "        return params, grads\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = load_data()\n",
        "    batch_size = 128\n",
        "    input_size = 64\n",
        "    hidden_size1 = 20\n",
        "    hidden_size2 = 20\n",
        "    output_size = 10\n",
        "    learning_rate = 0.005\n",
        "    epochs = 100\n",
        "\n",
        "    print(\"Training with SGD LR 0.005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size1,\n",
        "                            hidden_size2, output_size)\n",
        "    test_losses_sgd, test_accuracies_sgd = train(network, data, SGD(\n",
        "        network.get_params_and_grads()[0], learning_rate), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with SGD LR 0.05\")\n",
        "    network = NeuralNetwork(input_size, hidden_size1,\n",
        "                            hidden_size2, output_size)\n",
        "    test_losses_sgd_b, test_accuracies_sgd_b = train(network, data, SGD(\n",
        "        network.get_params_and_grads()[0], learning_rate * 10), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with SGD with Momentum LR 0.0005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size1,\n",
        "                            hidden_size2, output_size)\n",
        "    test_losses_momentum, test_accuracies_momentum = train(network, data, SGD_Momentum(\n",
        "        network.get_params_and_grads()[0], learning_rate / 10, 0.9), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with AdaGrad LR 0.005\")\n",
        "    network = NeuralNetwork(input_size, hidden_size1,\n",
        "                            hidden_size2, output_size)\n",
        "    test_losses_adagrad, test_accuracies_adagrad = train(network, data, SGD_AdaGrad(\n",
        "        network.get_params_and_grads()[0], learning_rate, 1e-8), epochs, batch_size)\n",
        "\n",
        "    print(\"Training with AdaGrad LR 0.05\")\n",
        "    network = NeuralNetwork(input_size, hidden_size1,\n",
        "                            hidden_size2, output_size)\n",
        "    test_losses_adagrad_b, test_accuracies_adagrad_b = train(network, data, SGD_AdaGrad(\n",
        "        network.get_params_and_grads()[0], learning_rate * 10, 1e-8), epochs, batch_size)\n",
        "\n",
        "    # Compare train losses and train accuracies\n",
        "    all_losses = [test_losses_sgd, test_losses_sgd_b,\n",
        "                  test_losses_momentum, test_losses_adagrad, test_losses_adagrad_b]\n",
        "    all_accuracies = [test_accuracies_sgd, test_accuracies_sgd_b,\n",
        "                      test_accuracies_momentum, test_accuracies_adagrad, test_accuracies_adagrad_b]\n",
        "    all_labels = [\"SGD LR 0.005\", \"SGD LR 0.05\",\n",
        "                  \"SGD-Momentum LR 0.0005\", \"AdaGrad LR 0.005\", \"AdaGrad LR 0.05\"]\n",
        "    plot_all_results(all_losses, all_accuracies, all_labels)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
