{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specified-graph",
   "metadata": {},
   "source": [
    "# Deep Q-learning (DQN)\n",
    "\n",
    "This notebook intends to demonstrate the DQN reinforcement learning (RL) algorithm. \n",
    "\n",
    "\n",
    "## Environment \n",
    "\n",
    "We will train an agent to balance a vertical pole on a cart. This is a classic environment in RL, named CartPole. See the environment description [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to get more details about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5298a26-4553-437e-9079-8328494be3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import utils.envs, utils.seed, utils.buffers, utils.torch, utils.common\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-digit",
   "metadata": {},
   "source": [
    "Initialize a set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9c1c-d26b-4ef5-a92e-e055b12bdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEEDS = [1,2,3]\n",
    "t = utils.torch.TorchHelper()\n",
    "DEVICE = t.device\n",
    "OBS_N = 4               # State space size\n",
    "ACT_N = 2               # Action space size\n",
    "MINIBATCH_SIZE = 10     # How many examples to sample per train step\n",
    "GAMMA = 0.99            # Discount factor in episodic reward objective\n",
    "LEARNING_RATE = 5e-4    # Learning rate for Adam optimizer\n",
    "TRAIN_AFTER_EPISODES = 10   # Just collect episodes for these many episodes\n",
    "TRAIN_EPOCHS = 5        # Train for these many epochs every time\n",
    "BUFSIZE = 10000         # Replay buffer size\n",
    "EPISODES = 300          # Total number of episodes to learn over\n",
    "TEST_EPISODES = 1       # Test episodes after every train episode\n",
    "HIDDEN = 512            # Hidden nodes\n",
    "TARGET_UPDATE_FREQ = 10 # Target network update frequency\n",
    "STARTING_EPSILON = 1.0  # Starting epsilon\n",
    "STEPS_MAX = 10000       # Gradually reduce epsilon over these many steps\n",
    "EPSILON_END = 0.01      # At the end, keep epsilon at this value\n",
    "\n",
    "# Global variables\n",
    "EPSILON = STARTING_EPSILON\n",
    "Q = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-emerald",
   "metadata": {},
   "source": [
    "Create the Cartpole Environment from Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "# Create replay buffer\n",
    "# Create network for Q(s, a)\n",
    "# Create target network\n",
    "# Create optimizer\n",
    "def create_everything(seed):\n",
    "\n",
    "    utils.seed.seed(seed)\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env.reset(seed=seed) ####\n",
    "    test_env = gym.make(\"CartPole-v0\")\n",
    "    test_env.reset(seed=10+seed) ####\n",
    "    buf = utils.buffers.ReplayBuffer(BUFSIZE)\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    Qt = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    OPT = torch.optim.Adam(Q.parameters(), lr = LEARNING_RATE)\n",
    "    return env, test_env, buf, Q, Qt, OPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-broadcasting",
   "metadata": {},
   "source": [
    "Implementing a function to update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a target network using a source network\n",
    "def update(target, source):\n",
    "    for tp, p in zip(target.parameters(), source.parameters()):\n",
    "        tp.data.copy_(p.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-light",
   "metadata": {},
   "source": [
    "Implementing a function for the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epsilon-greedy policy\n",
    "def policy(env, obs):\n",
    "\n",
    "    global EPSILON, Q\n",
    "\n",
    "    obs = t.f(obs).view(-1, OBS_N)  # Convert to torch tensor\n",
    "    \n",
    "    # With probability EPSILON, choose a random action\n",
    "    # Rest of the time, choose argmax_a Q(s, a) \n",
    "    if np.random.rand() < EPSILON:\n",
    "        action = np.random.randint(ACT_N)\n",
    "    else:\n",
    "        qvalues = Q(obs)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    \n",
    "    # Epsilon update rule: Keep reducing a small amount over\n",
    "    # STEPS_MAX number of steps, and at the end, fix to EPSILON_END\n",
    "    EPSILON = max(EPSILON_END, EPSILON - (1.0 / STEPS_MAX))\n",
    "    # print(EPSILON)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-billion",
   "metadata": {},
   "source": [
    "Function to update the evaluation network (and target network through the previous update function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update networks\n",
    "def update_networks(epi, buf, Q, Qt, OPT):\n",
    "    \n",
    "    # Sample a minibatch (s, a, r, s', d)\n",
    "    # Each variable is a vector of corresponding values\n",
    "    S, A, R, S2, D = buf.sample(MINIBATCH_SIZE, t)\n",
    "    \n",
    "    # Get Q(s, a) for every (s, a) in the minibatch\n",
    "    qvalues = Q(S).gather(1, A.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Get max_a' Qt(s', a') for every (s') in the minibatch\n",
    "    q2values = torch.max(Qt(S2), dim = 1).values\n",
    "\n",
    "    # If done, \n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (0)\n",
    "    # If not done,\n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (1)       \n",
    "    targets = R + GAMMA * q2values * (1-D)\n",
    "\n",
    "    # Detach y since it is the target. Target values should\n",
    "    # be kept fixed.\n",
    "    loss = torch.nn.MSELoss()(targets.detach(), qvalues)\n",
    "\n",
    "    # Backpropagation\n",
    "    OPT.zero_grad()\n",
    "    loss.backward()\n",
    "    OPT.step()\n",
    "\n",
    "    # Update target network every few steps\n",
    "    if epi % TARGET_UPDATE_FREQ == 0:\n",
    "        update(Qt, Q)\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-tanzania",
   "metadata": {},
   "source": [
    "Train function to train a DQN agent in Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play episodes\n",
    "# Training function\n",
    "def train(seed):\n",
    "\n",
    "    global EPSILON, Q\n",
    "    print(\"Seed=%d\" % seed)\n",
    "\n",
    "    # Create environment, buffer, Q, Q target, optimizer\n",
    "    env, test_env, buf, Q, Qt, OPT = create_everything(seed)\n",
    "\n",
    "    # epsilon greedy exploration\n",
    "    EPSILON = STARTING_EPSILON\n",
    "\n",
    "    testRs = []\n",
    "    last25testRs = []\n",
    "    print(\"Training:\")\n",
    "    pbar = tqdm.trange(EPISODES)\n",
    "    for epi in pbar:\n",
    "\n",
    "        # Play an episode and log episodic reward\n",
    "        S, A, R = utils.envs.play_episode_rb(env, policy, buf)\n",
    "        \n",
    "        # Train after collecting sufficient experience\n",
    "        if epi >= TRAIN_AFTER_EPISODES:\n",
    "\n",
    "            # Train for TRAIN_EPOCHS\n",
    "            for tri in range(TRAIN_EPOCHS): \n",
    "                update_networks(epi, buf, Q, Qt, OPT)\n",
    "\n",
    "        # Evaluate for TEST_EPISODES number of episodes\n",
    "        Rews = []\n",
    "        for epj in range(TEST_EPISODES):\n",
    "            S, A, R = utils.envs.play_episode(test_env, policy, render = False)\n",
    "            Rews += [sum(R)]\n",
    "        testRs += [sum(Rews)/TEST_EPISODES]\n",
    "\n",
    "        # Update progress bar\n",
    "        last25testRs += [sum(testRs[-25:])/len(testRs[-25:])]\n",
    "        pbar.set_description(\"R25(%g)\" % (last25testRs[-1]))\n",
    "\n",
    "    # Close progress bar, environment\n",
    "    pbar.close()\n",
    "    print(\"Training finished!\")\n",
    "    env.close()\n",
    "\n",
    "    return last25testRs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-barrel",
   "metadata": {},
   "source": [
    "Running training and plotting performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean curve and (mean-std, mean+std) curve with some transparency\n",
    "# Clip the curves to be between 0, 200\n",
    "def plot_arrays(vars, color, label):\n",
    "    mean = np.mean(vars, axis=0)\n",
    "    std = np.std(vars, axis=0)\n",
    "    plt.plot(range(len(mean)), mean, color=color, label=label)\n",
    "    plt.fill_between(range(len(mean)), np.maximum(mean-std, 0), np.minimum(mean+std,200), color=color, alpha=0.3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Train for different seeds\n",
    "    curves = []\n",
    "    for seed in SEEDS:\n",
    "        curves += [train(seed)]\n",
    "\n",
    "    # Plot the curve for the given seeds\n",
    "    plot_arrays(curves, 'b', 'dqn')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward (averaged over last 25 episodes)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-effectiveness",
   "metadata": {},
   "source": [
    "## Outcomes of this module \n",
    "\n",
    "Gain a working understanding of: \n",
    "\n",
    "- Value based approach in RL\n",
    "\n",
    "- Q-learning algorithm\n",
    "\n",
    "- Function approximation technique (using a neural network) \n",
    "\n",
    "- DQN algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementLearning-Bootcamp",
   "language": "python",
   "name": "rl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
